//go:build !noasm && amd64
// Code generated by gocc rev-e26dddf -- DO NOT EDIT.
//
// Source file         : zigzag.c
// Clang version       : Apple clang version 16.0.0 (clang-1600.0.26.6)
// Target architecture : amd64
// Compiler options    : -mavx2 -mfma

#include "textflag.h"

TEXT ·zigzag_encode_avx2(SB), NOSPLIT, $0-24
	MOVQ in+0(FP), DI
	MOVQ out+8(FP), SI
	MOVQ N+16(FP), DX
	WORD $0x8548; BYTE $0xd2 // TESTQ DX, DX                         // test	rdx, rdx
	JE   LBB0_11             // <--                                  // je	.LBB0_11
	NOP                      // (skipped)                            // push	rbp
	NOP                      // (skipped)                            // mov	rbp, rsp
	NOP                      // (skipped)                            // and	rsp, -8
	XORL AX, AX              // <--                                  // xor	eax, eax
	CMPQ DX, $0x20           // <--                                  // cmp	rdx, 32
	JB   LBB0_6              // <--                                  // jb	.LBB0_6
	MOVQ SI, CX              // <--                                  // mov	rcx, rsi
	SUBQ DI, CX              // <--                                  // sub	rcx, rdi
	CMPQ CX, $0x80           // <--                                  // cmp	rcx, 128
	JB   LBB0_6              // <--                                  // jb	.LBB0_6
	MOVQ DX, AX              // <--                                  // mov	rax, rdx
	ANDQ $-0x20, AX          // <--                                  // and	rax, -32
	XORL CX, CX              // <--                                  // xor	ecx, ecx

LBB0_4:
	LONG $0x046ffec5; BYTE $0x8f   // VMOVDQU 0(DI)(CX*4), Y0              // vmovdqu	ymm0, ymmword ptr [rdi + 4*rcx]
	LONG $0x4c6ffec5; WORD $0x208f // VMOVDQU 0x20(DI)(CX*4), Y1           // vmovdqu	ymm1, ymmword ptr [rdi + 4*rcx + 32]
	LONG $0x546ffec5; WORD $0x408f // VMOVDQU 0x40(DI)(CX*4), Y2           // vmovdqu	ymm2, ymmword ptr [rdi + 4*rcx + 64]
	LONG $0x5c6ffec5; WORD $0x608f // VMOVDQU 0x60(DI)(CX*4), Y3           // vmovdqu	ymm3, ymmword ptr [rdi + 4*rcx + 96]
	LONG $0xe0fefdc5               // VPADDD Y0, Y0, Y4                    // vpaddd	ymm4, ymm0, ymm0
	LONG $0xe9fef5c5               // VPADDD Y1, Y1, Y5                    // vpaddd	ymm5, ymm1, ymm1
	LONG $0xf2feedc5               // VPADDD Y2, Y2, Y6                    // vpaddd	ymm6, ymm2, ymm2
	LONG $0xfbfee5c5               // VPADDD Y3, Y3, Y7                    // vpaddd	ymm7, ymm3, ymm3
	LONG $0xe072fdc5; BYTE $0x1f   // VPSRAD $0x1f, Y0, Y0                 // vpsrad	ymm0, ymm0, 31
	LONG $0xc0efddc5               // VPXOR Y0, Y4, Y0                     // vpxor	ymm0, ymm4, ymm0
	LONG $0xe172f5c5; BYTE $0x1f   // VPSRAD $0x1f, Y1, Y1                 // vpsrad	ymm1, ymm1, 31
	LONG $0xc9efd5c5               // VPXOR Y1, Y5, Y1                     // vpxor	ymm1, ymm5, ymm1
	LONG $0xe272edc5; BYTE $0x1f   // VPSRAD $0x1f, Y2, Y2                 // vpsrad	ymm2, ymm2, 31
	LONG $0xd2efcdc5               // VPXOR Y2, Y6, Y2                     // vpxor	ymm2, ymm6, ymm2
	LONG $0xe372e5c5; BYTE $0x1f   // VPSRAD $0x1f, Y3, Y3                 // vpsrad	ymm3, ymm3, 31
	LONG $0xdbefc5c5               // VPXOR Y3, Y7, Y3                     // vpxor	ymm3, ymm7, ymm3
	LONG $0x047ffec5; BYTE $0x8e   // VMOVDQU Y0, 0(SI)(CX*4)              // vmovdqu	ymmword ptr [rsi + 4*rcx], ymm0
	LONG $0x4c7ffec5; WORD $0x208e // VMOVDQU Y1, 0x20(SI)(CX*4)           // vmovdqu	ymmword ptr [rsi + 4*rcx + 32], ymm1
	LONG $0x547ffec5; WORD $0x408e // VMOVDQU Y2, 0x40(SI)(CX*4)           // vmovdqu	ymmword ptr [rsi + 4*rcx + 64], ymm2
	LONG $0x5c7ffec5; WORD $0x608e // VMOVDQU Y3, 0x60(SI)(CX*4)           // vmovdqu	ymmword ptr [rsi + 4*rcx + 96], ymm3
	ADDQ $0x20, CX                 // <--                                  // add	rcx, 32
	CMPQ AX, CX                    // <--                                  // cmp	rax, rcx
	JNE  LBB0_4                    // <--                                  // jne	.LBB0_4
	CMPQ AX, DX                    // <--                                  // cmp	rax, rdx
	JE   LBB0_10                   // <--                                  // je	.LBB0_10

LBB0_6:
	MOVQ AX, CX              // <--                                  // mov	rcx, rax
	NOTQ CX                  // <--                                  // not	rcx
	WORD $0xc2f6; BYTE $0x01 // TESTL $0x1, DL                       // test	dl, 1
	JE   LBB0_8              // <--                                  // je	.LBB0_8
	LONG $0x87048b44         // MOVL 0(DI)(AX*4), R8                 // mov	r8d, dword ptr [rdi + 4*rax]
	LONG $0x000c8d47         // LEAL 0(R8)(R8*1), R9                 // lea	r9d, [r8 + r8]
	LONG $0x1ff8c141         // SARL $0x1f, R8                       // sar	r8d, 31
	XORL R9, R8              // <--                                  // xor	r8d, r9d
	LONG $0x86048944         // MOVL R8, 0(SI)(AX*4)                 // mov	dword ptr [rsi + 4*rax], r8d
	ORQ  $0x1, AX            // <--                                  // or	rax, 1

LBB0_8:
	ADDQ DX, CX  // <--                                  // add	rcx, rdx
	JE   LBB0_10 // <--                                  // je	.LBB0_10

LBB0_9:
	WORD $0x0c8b; BYTE $0x87 // MOVL 0(DI)(AX*4), CX                 // mov	ecx, dword ptr [rdi + 4*rax]
	LONG $0x09048d44         // LEAL 0(CX)(CX*1), R8                 // lea	r8d, [rcx + rcx]
	WORD $0xf9c1; BYTE $0x1f // SARL $0x1f, CX                       // sar	ecx, 31
	XORL R8, CX              // <--                                  // xor	ecx, r8d
	WORD $0x0c89; BYTE $0x86 // MOVL CX, 0(SI)(AX*4)                 // mov	dword ptr [rsi + 4*rax], ecx
	LONG $0x04874c8b         // MOVL 0x4(DI)(AX*4), CX               // mov	ecx, dword ptr [rdi + 4*rax + 4]
	LONG $0x09048d44         // LEAL 0(CX)(CX*1), R8                 // lea	r8d, [rcx + rcx]
	WORD $0xf9c1; BYTE $0x1f // SARL $0x1f, CX                       // sar	ecx, 31
	XORL R8, CX              // <--                                  // xor	ecx, r8d
	LONG $0x04864c89         // MOVL CX, 0x4(SI)(AX*4)               // mov	dword ptr [rsi + 4*rax + 4], ecx
	ADDQ $0x2, AX            // <--                                  // add	rax, 2
	CMPQ DX, AX              // <--                                  // cmp	rdx, rax
	JNE  LBB0_9              // <--                                  // jne	.LBB0_9

LBB0_10:
	NOP // (skipped)                            // mov	rsp, rbp
	NOP // (skipped)                            // pop	rbp

LBB0_11:
	VZEROUPPER // <--                                  // vzeroupper
	RET        // <--                                  // ret

TEXT ·zigzag_delta_encode_avx2(SB), NOSPLIT, $0-28
	MOVQ    in+0(FP), DI
	MOVQ    out+8(FP), SI
	MOVQ    N+16(FP), DX
	MOVLQZX prev+24(FP), CX
	WORD    $0x8548; BYTE $0xd2 // TESTQ DX, DX                         // test	rdx, rdx
	JE      LBB1_6              // <--                                  // je	.LBB1_6
	NOP                         // (skipped)                            // push	rbp
	NOP                         // (skipped)                            // mov	rbp, rsp
	NOP                         // (skipped)                            // and	rsp, -8
	CMPQ    DX, $0x1            // <--                                  // cmp	rdx, 1
	JNE     LBB1_7              // <--                                  // jne	.LBB1_7
	XORL    AX, AX              // <--                                  // xor	eax, eax
	JMP     LBB1_3              // <--                                  // jmp	.LBB1_3

LBB1_7:
	MOVQ DX, R8    // <--                                  // mov	r8, rdx
	ANDQ $-0x2, R8 // <--                                  // and	r8, -2
	XORL AX, AX    // <--                                  // xor	eax, eax

LBB1_8:
	LONG $0x870c8b44         // MOVL 0(DI)(AX*4), R9                 // mov	r9d, dword ptr [rdi + 4*rax]
	WORD $0x2941; BYTE $0xc9 // SUBL CX, R9                          // sub	r9d, ecx
	LONG $0x090c8d43         // LEAL 0(R9)(R9*1), CX                 // lea	ecx, [r9 + r9]
	LONG $0x1ff9c141         // SARL $0x1f, R9                       // sar	r9d, 31
	XORL CX, R9              // <--                                  // xor	r9d, ecx
	LONG $0x860c8944         // MOVL R9, 0(SI)(AX*4)                 // mov	dword ptr [rsi + 4*rax], r9d
	LONG $0x04874c8b         // MOVL 0x4(DI)(AX*4), CX               // mov	ecx, dword ptr [rdi + 4*rax + 4]
	WORD $0x0c2b; BYTE $0x87 // SUBL 0(DI)(AX*4), CX                 // sub	ecx, dword ptr [rdi + 4*rax]
	LONG $0x090c8d44         // LEAL 0(CX)(CX*1), R9                 // lea	r9d, [rcx + rcx]
	WORD $0xf9c1; BYTE $0x1f // SARL $0x1f, CX                       // sar	ecx, 31
	XORL R9, CX              // <--                                  // xor	ecx, r9d
	LONG $0x04864c89         // MOVL CX, 0x4(SI)(AX*4)               // mov	dword ptr [rsi + 4*rax + 4], ecx
	LONG $0x04874c8b         // MOVL 0x4(DI)(AX*4), CX               // mov	ecx, dword ptr [rdi + 4*rax + 4]
	ADDQ $0x2, AX            // <--                                  // add	rax, 2
	CMPQ R8, AX              // <--                                  // cmp	r8, rax
	JNE  LBB1_8              // <--                                  // jne	.LBB1_8

LBB1_3:
	WORD $0xc2f6; BYTE $0x01 // TESTL $0x1, DL                       // test	dl, 1
	JE   LBB1_5              // <--                                  // je	.LBB1_5
	WORD $0x148b; BYTE $0x87 // MOVL 0(DI)(AX*4), DX                 // mov	edx, dword ptr [rdi + 4*rax]
	WORD $0xca29             // SUBL CX, DX                          // sub	edx, ecx
	WORD $0x0c8d; BYTE $0x12 // LEAL 0(DX)(DX*1), CX                 // lea	ecx, [rdx + rdx]
	WORD $0xfac1; BYTE $0x1f // SARL $0x1f, DX                       // sar	edx, 31
	XORL CX, DX              // <--                                  // xor	edx, ecx
	WORD $0x1489; BYTE $0x86 // MOVL DX, 0(SI)(AX*4)                 // mov	dword ptr [rsi + 4*rax], edx

LBB1_5:
	NOP // (skipped)                            // mov	rsp, rbp
	NOP // (skipped)                            // pop	rbp

LBB1_6:
	RET // <--                                  // ret

DATA LCPI2_0<>+0x00(SB)/4, $0x00000001
GLOBL LCPI2_0<>(SB), (RODATA|NOPTR), $4

TEXT ·zigzag_decode_avx2(SB), NOSPLIT, $0-24
	MOVQ         in+0(FP), DI
	MOVQ         out+8(FP), SI
	MOVQ         N+16(FP), DX
	WORD         $0x8548; BYTE $0xd2 // TESTQ DX, DX                         // test	rdx, rdx
	JE           LBB2_11             // <--                                  // je	.LBB2_11
	NOP                              // (skipped)                            // push	rbp
	NOP                              // (skipped)                            // mov	rbp, rsp
	NOP                              // (skipped)                            // and	rsp, -8
	XORL         AX, AX              // <--                                  // xor	eax, eax
	CMPQ         DX, $0x20           // <--                                  // cmp	rdx, 32
	JB           LBB2_6              // <--                                  // jb	.LBB2_6
	MOVQ         SI, CX              // <--                                  // mov	rcx, rsi
	SUBQ         DI, CX              // <--                                  // sub	rcx, rdi
	CMPQ         CX, $0x80           // <--                                  // cmp	rcx, 128
	JB           LBB2_6              // <--                                  // jb	.LBB2_6
	MOVQ         DX, AX              // <--                                  // mov	rax, rdx
	ANDQ         $-0x20, AX          // <--                                  // and	rax, -32
	XORL         CX, CX              // <--                                  // xor	ecx, ecx
	VPBROADCASTD LCPI2_0<>(SB), Y0   // <--                                  // vpbroadcastd	ymm0, dword ptr [rip + .LCPI2_0]
	LONG         $0xc9eff1c5         // VPXOR X1, X1, X1                     // vpxor	xmm1, xmm1, xmm1

LBB2_4:
	LONG $0x146ffec5; BYTE $0x8f   // VMOVDQU 0(DI)(CX*4), Y2              // vmovdqu	ymm2, ymmword ptr [rdi + 4*rcx]
	LONG $0x5c6ffec5; WORD $0x208f // VMOVDQU 0x20(DI)(CX*4), Y3           // vmovdqu	ymm3, ymmword ptr [rdi + 4*rcx + 32]
	LONG $0x646ffec5; WORD $0x408f // VMOVDQU 0x40(DI)(CX*4), Y4           // vmovdqu	ymm4, ymmword ptr [rdi + 4*rcx + 64]
	LONG $0x6c6ffec5; WORD $0x608f // VMOVDQU 0x60(DI)(CX*4), Y5           // vmovdqu	ymm5, ymmword ptr [rdi + 4*rcx + 96]
	LONG $0xd272cdc5; BYTE $0x01   // ?                                    // vpsrld	ymm6, ymm2, 1
	LONG $0xd372c5c5; BYTE $0x01   // ?                                    // vpsrld	ymm7, ymm3, 1
	LONG $0xd472bdc5; BYTE $0x01   // ?                                    // vpsrld	ymm8, ymm4, 1
	LONG $0xd572b5c5; BYTE $0x01   // ?                                    // vpsrld	ymm9, ymm5, 1
	LONG $0xd0dbedc5               // VPAND Y0, Y2, Y2                     // vpand	ymm2, ymm2, ymm0
	LONG $0xd8dbe5c5               // VPAND Y0, Y3, Y3                     // vpand	ymm3, ymm3, ymm0
	LONG $0xe0dbddc5               // VPAND Y0, Y4, Y4                     // vpand	ymm4, ymm4, ymm0
	LONG $0xe8dbd5c5               // VPAND Y0, Y5, Y5                     // vpand	ymm5, ymm5, ymm0
	LONG $0xd2faf5c5               // VPSUBD Y2, Y1, Y2                    // vpsubd	ymm2, ymm1, ymm2
	LONG $0xd2efcdc5               // VPXOR Y2, Y6, Y2                     // vpxor	ymm2, ymm6, ymm2
	LONG $0xdbfaf5c5               // VPSUBD Y3, Y1, Y3                    // vpsubd	ymm3, ymm1, ymm3
	LONG $0xdbefc5c5               // VPXOR Y3, Y7, Y3                     // vpxor	ymm3, ymm7, ymm3
	LONG $0xe4faf5c5               // VPSUBD Y4, Y1, Y4                    // vpsubd	ymm4, ymm1, ymm4
	LONG $0xe4efbdc5               // VPXOR Y4, Y8, Y4                     // vpxor	ymm4, ymm8, ymm4
	LONG $0xedfaf5c5               // VPSUBD Y5, Y1, Y5                    // vpsubd	ymm5, ymm1, ymm5
	LONG $0xedefb5c5               // VPXOR Y5, Y9, Y5                     // vpxor	ymm5, ymm9, ymm5
	LONG $0x147ffec5; BYTE $0x8e   // VMOVDQU Y2, 0(SI)(CX*4)              // vmovdqu	ymmword ptr [rsi + 4*rcx], ymm2
	LONG $0x5c7ffec5; WORD $0x208e // VMOVDQU Y3, 0x20(SI)(CX*4)           // vmovdqu	ymmword ptr [rsi + 4*rcx + 32], ymm3
	LONG $0x647ffec5; WORD $0x408e // VMOVDQU Y4, 0x40(SI)(CX*4)           // vmovdqu	ymmword ptr [rsi + 4*rcx + 64], ymm4
	LONG $0x6c7ffec5; WORD $0x608e // VMOVDQU Y5, 0x60(SI)(CX*4)           // vmovdqu	ymmword ptr [rsi + 4*rcx + 96], ymm5
	ADDQ $0x20, CX                 // <--                                  // add	rcx, 32
	CMPQ AX, CX                    // <--                                  // cmp	rax, rcx
	JNE  LBB2_4                    // <--                                  // jne	.LBB2_4
	CMPQ AX, DX                    // <--                                  // cmp	rax, rdx
	JE   LBB2_10                   // <--                                  // je	.LBB2_10

LBB2_6:
	MOVQ AX, CX              // <--                                  // mov	rcx, rax
	NOTQ CX                  // <--                                  // not	rcx
	WORD $0xc2f6; BYTE $0x01 // TESTL $0x1, DL                       // test	dl, 1
	JE   LBB2_8              // <--                                  // je	.LBB2_8
	LONG $0x87048b44         // MOVL 0(DI)(AX*4), R8                 // mov	r8d, dword ptr [rdi + 4*rax]
	WORD $0x8945; BYTE $0xc1 // MOVL R8, R9                          // mov	r9d, r8d
	WORD $0xd141; BYTE $0xe9 // SHRL $0x1, R9                        // shr	r9d
	LONG $0x01e08341         // ANDL $0x1, R8                        // and	r8d, 1
	WORD $0xf741; BYTE $0xd8 // NEGL R8                              // neg	r8d
	XORL R9, R8              // <--                                  // xor	r8d, r9d
	LONG $0x86048944         // MOVL R8, 0(SI)(AX*4)                 // mov	dword ptr [rsi + 4*rax], r8d
	ORQ  $0x1, AX            // <--                                  // or	rax, 1

LBB2_8:
	ADDQ DX, CX  // <--                                  // add	rcx, rdx
	JE   LBB2_10 // <--                                  // je	.LBB2_10

LBB2_9:
	WORD $0x0c8b; BYTE $0x87 // MOVL 0(DI)(AX*4), CX                 // mov	ecx, dword ptr [rdi + 4*rax]
	WORD $0x8941; BYTE $0xc8 // MOVL CX, R8                          // mov	r8d, ecx
	WORD $0xd141; BYTE $0xe8 // SHRL $0x1, R8                        // shr	r8d
	WORD $0xe183; BYTE $0x01 // ANDL $0x1, CX                        // and	ecx, 1
	WORD $0xd9f7             // NEGL CX                              // neg	ecx
	XORL R8, CX              // <--                                  // xor	ecx, r8d
	WORD $0x0c89; BYTE $0x86 // MOVL CX, 0(SI)(AX*4)                 // mov	dword ptr [rsi + 4*rax], ecx
	LONG $0x04874c8b         // MOVL 0x4(DI)(AX*4), CX               // mov	ecx, dword ptr [rdi + 4*rax + 4]
	WORD $0x8941; BYTE $0xc8 // MOVL CX, R8                          // mov	r8d, ecx
	WORD $0xd141; BYTE $0xe8 // SHRL $0x1, R8                        // shr	r8d
	WORD $0xe183; BYTE $0x01 // ANDL $0x1, CX                        // and	ecx, 1
	WORD $0xd9f7             // NEGL CX                              // neg	ecx
	XORL R8, CX              // <--                                  // xor	ecx, r8d
	LONG $0x04864c89         // MOVL CX, 0x4(SI)(AX*4)               // mov	dword ptr [rsi + 4*rax + 4], ecx
	ADDQ $0x2, AX            // <--                                  // add	rax, 2
	CMPQ DX, AX              // <--                                  // cmp	rdx, rax
	JNE  LBB2_9              // <--                                  // jne	.LBB2_9

LBB2_10:
	NOP // (skipped)                            // mov	rsp, rbp
	NOP // (skipped)                            // pop	rbp

LBB2_11:
	VZEROUPPER // <--                                  // vzeroupper
	RET        // <--                                  // ret

TEXT ·zigzag_delta_decode_avx2(SB), NOSPLIT, $0-28
	MOVQ    in+0(FP), DI
	MOVQ    out+8(FP), SI
	MOVQ    N+16(FP), DX
	MOVLQZX prev+24(FP), CX
	WORD    $0x8548; BYTE $0xd2 // TESTQ DX, DX                         // test	rdx, rdx
	JE      LBB3_6              // <--                                  // je	.LBB3_6
	NOP                         // (skipped)                            // push	rbp
	NOP                         // (skipped)                            // mov	rbp, rsp
	NOP                         // (skipped)                            // and	rsp, -8
	CMPQ    DX, $0x1            // <--                                  // cmp	rdx, 1
	JNE     LBB3_7              // <--                                  // jne	.LBB3_7
	XORL    AX, AX              // <--                                  // xor	eax, eax
	JMP     LBB3_3              // <--                                  // jmp	.LBB3_3

LBB3_7:
	MOVQ DX, R8    // <--                                  // mov	r8, rdx
	ANDQ $-0x2, R8 // <--                                  // and	r8, -2
	XORL AX, AX    // <--                                  // xor	eax, eax

LBB3_8:
	LONG $0x870c8b44         // MOVL 0(DI)(AX*4), R9                 // mov	r9d, dword ptr [rdi + 4*rax]
	WORD $0x8945; BYTE $0xca // MOVL R9, R10                         // mov	r10d, r9d
	WORD $0xd141; BYTE $0xea // SHRL $0x1, R10                       // shr	r10d
	LONG $0x01e18341         // ANDL $0x1, R9                        // and	r9d, 1
	WORD $0xf741; BYTE $0xd9 // NEGL R9                              // neg	r9d
	XORL R10, R9             // <--                                  // xor	r9d, r10d
	WORD $0x0141; BYTE $0xc9 // ADDL CX, R9                          // add	r9d, ecx
	LONG $0x860c8944         // MOVL R9, 0(SI)(AX*4)                 // mov	dword ptr [rsi + 4*rax], r9d
	LONG $0x04874c8b         // MOVL 0x4(DI)(AX*4), CX               // mov	ecx, dword ptr [rdi + 4*rax + 4]
	WORD $0x8941; BYTE $0xca // MOVL CX, R10                         // mov	r10d, ecx
	WORD $0xd141; BYTE $0xea // SHRL $0x1, R10                       // shr	r10d
	WORD $0xe183; BYTE $0x01 // ANDL $0x1, CX                        // and	ecx, 1
	WORD $0xd9f7             // NEGL CX                              // neg	ecx
	XORL R10, CX             // <--                                  // xor	ecx, r10d
	WORD $0x0144; BYTE $0xc9 // ADDL R9, CX                          // add	ecx, r9d
	LONG $0x04864c89         // MOVL CX, 0x4(SI)(AX*4)               // mov	dword ptr [rsi + 4*rax + 4], ecx
	ADDQ $0x2, AX            // <--                                  // add	rax, 2
	CMPQ R8, AX              // <--                                  // cmp	r8, rax
	JNE  LBB3_8              // <--                                  // jne	.LBB3_8

LBB3_3:
	WORD $0xc2f6; BYTE $0x01 // TESTL $0x1, DL                       // test	dl, 1
	JE   LBB3_5              // <--                                  // je	.LBB3_5
	WORD $0x148b; BYTE $0x87 // MOVL 0(DI)(AX*4), DX                 // mov	edx, dword ptr [rdi + 4*rax]
	WORD $0xd789             // MOVL DX, DI                          // mov	edi, edx
	WORD $0xefd1             // SHRL $0x1, DI                        // shr	edi
	WORD $0xe283; BYTE $0x01 // ANDL $0x1, DX                        // and	edx, 1
	WORD $0xdaf7             // NEGL DX                              // neg	edx
	XORL DI, DX              // <--                                  // xor	edx, edi
	WORD $0xca01             // ADDL CX, DX                          // add	edx, ecx
	WORD $0x1489; BYTE $0x86 // MOVL DX, 0(SI)(AX*4)                 // mov	dword ptr [rsi + 4*rax], edx

LBB3_5:
	NOP // (skipped)                            // mov	rsp, rbp
	NOP // (skipped)                            // pop	rbp

LBB3_6:
	RET // <--                                  // ret
